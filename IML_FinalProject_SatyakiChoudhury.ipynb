{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNN+vLnU42ISS5UeU0989Ze",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satbit007/Machine-Learning-Driven-Approach-to-Automated-Essay-Scoring-AES-/blob/main/IML_FinalProject_SatyakiChoudhury.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Necessary Libraries"
      ],
      "metadata": {
        "id": "O7cbOOVu2Vo0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwStSwOa1djU",
        "outputId": "5d353915-e662-4b4c-d2df-b180f4fc3c58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and Preprocess the Data"
      ],
      "metadata": {
        "id": "x09YvwE52Ubd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "train_df = pd.read_csv('/content/training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "\n",
        "# Load validation data\n",
        "valid_df = pd.read_csv('/content/valid_set.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "\n",
        "# After loading the training data\n",
        "print(\"Column names in training data:\", train_df.columns)\n",
        "\n",
        "# After loading the validation data\n",
        "print(\"Column names in validation data:\", valid_df.columns)\n",
        "\n",
        "# Display the first few rows of the training data\n",
        "print(train_df.head())\n",
        "\n",
        "# Display the first few rows of the validation data\n",
        "print(valid_df.head())\n",
        "\n",
        "# Basic preprocessing\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuations\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "    return text\n",
        "\n",
        "train_df['essay'] = train_df['essay'].apply(preprocess_text)\n",
        "valid_df['essay'] = valid_df['essay'].apply(preprocess_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpu2Bzn52gBe",
        "outputId": "b8e57646-ba59-47c0-a074-da3f61434497"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names in training data: Index(['essay_id', 'essay_set', 'essay', 'rater1_domain1', 'rater2_domain1',\n",
            "       'rater3_domain1', 'domain1_score', 'rater1_domain2', 'rater2_domain2',\n",
            "       'domain2_score', 'rater1_trait1', 'rater1_trait2', 'rater1_trait3',\n",
            "       'rater1_trait4', 'rater1_trait5', 'rater1_trait6', 'rater2_trait1',\n",
            "       'rater2_trait2', 'rater2_trait3', 'rater2_trait4', 'rater2_trait5',\n",
            "       'rater2_trait6', 'rater3_trait1', 'rater3_trait2', 'rater3_trait3',\n",
            "       'rater3_trait4', 'rater3_trait5', 'rater3_trait6'],\n",
            "      dtype='object')\n",
            "Column names in validation data: Index(['essay_id', 'essay_set', 'essay', 'domain1_predictionid',\n",
            "       'domain2_predictionid'],\n",
            "      dtype='object')\n",
            "   essay_id  essay_set                                              essay  \\\n",
            "0         1          1  Dear local newspaper, I think effects computer...   \n",
            "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
            "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
            "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
            "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
            "\n",
            "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
            "0               4               4             NaN              8   \n",
            "1               5               4             NaN              9   \n",
            "2               4               3             NaN              7   \n",
            "3               5               5             NaN             10   \n",
            "4               4               4             NaN              8   \n",
            "\n",
            "   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n",
            "0             NaN             NaN            NaN  ...            NaN   \n",
            "1             NaN             NaN            NaN  ...            NaN   \n",
            "2             NaN             NaN            NaN  ...            NaN   \n",
            "3             NaN             NaN            NaN  ...            NaN   \n",
            "4             NaN             NaN            NaN  ...            NaN   \n",
            "\n",
            "   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n",
            "0            NaN            NaN            NaN            NaN            NaN   \n",
            "1            NaN            NaN            NaN            NaN            NaN   \n",
            "2            NaN            NaN            NaN            NaN            NaN   \n",
            "3            NaN            NaN            NaN            NaN            NaN   \n",
            "4            NaN            NaN            NaN            NaN            NaN   \n",
            "\n",
            "   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
            "0            NaN            NaN            NaN            NaN  \n",
            "1            NaN            NaN            NaN            NaN  \n",
            "2            NaN            NaN            NaN            NaN  \n",
            "3            NaN            NaN            NaN            NaN  \n",
            "4            NaN            NaN            NaN            NaN  \n",
            "\n",
            "[5 rows x 28 columns]\n",
            "   essay_id  essay_set                                              essay  \\\n",
            "0      1788          1  Dear @ORGANIZATION1, @CAPS1 more and more peop...   \n",
            "1      1789          1  Dear @LOCATION1 Time @CAPS1 me tell you what I...   \n",
            "2      1790          1  Dear Local newspaper, Have you been spending a...   \n",
            "3      1791          1  Dear Readers, @CAPS1 you imagine how life woul...   \n",
            "4      1792          1  Dear newspaper, I strongly believe that comput...   \n",
            "\n",
            "   domain1_predictionid  domain2_predictionid  \n",
            "0                  1788                   NaN  \n",
            "1                  1789                   NaN  \n",
            "2                  1790                   NaN  \n",
            "3                  1791                   NaN  \n",
            "4                  1792                   NaN  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction"
      ],
      "metadata": {
        "id": "_s2aIWE12oSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(stop_words=stopwords.words('english'), max_features=5000)\n",
        "X_train_tfidf = tfidf.fit_transform(train_df['essay'])\n",
        "X_valid_tfidf = tfidf.transform(valid_df['essay'])\n",
        "\n",
        "# Prepare target variable for training data\n",
        "y_train = train_df['domain1_score']\n",
        "\n",
        "# Right after creating X_train_tfidf and y_train\n",
        "print(\"Number of rows in the feature set:\", X_train_tfidf.shape[0])\n",
        "print(\"Number of rows in the target variable:\", y_train.shape[0])\n",
        "\n",
        "\n",
        "# If you decide to use a part of the training data for validation\n",
        "X_train_tfidf, X_val_tfidf, y_train, y_val = train_test_split(X_train_tfidf, y_train, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69R3JE4g2n-w",
        "outputId": "d04ee9bc-768c-4668-e4b6-05308c615881"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the feature set: 12976\n",
            "Number of rows in the target variable: 12976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training"
      ],
      "metadata": {
        "id": "IW-Koyc12sZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Assuming 'domain1_score' as the target for training data\n",
        "y_train = train_df['domain1_score']\n",
        "\n",
        "# Split the training data into new training and validation subsets\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_tfidf, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the RandomForestRegressor model on the new training subset\n",
        "model = RandomForestRegressor(n_estimators=100)\n",
        "model.fit(X_train_split, y_train_split)\n",
        "\n",
        "# Predict and evaluate the model on the new validation subset\n",
        "y_pred_val = model.predict(X_val_split)\n",
        "mse = mean_squared_error(y_val_split, y_pred_val)\n",
        "r2 = r2_score(y_val_split, y_pred_val)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Squared Error on validation set: {mse}')\n",
        "print(f'R^2 Score on validation set: {r2}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "1yoUOT9R2wQy",
        "outputId": "ed0f4e2f-7219-499e-a907-71c2d7ea3688"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1d5ff7223913>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Split the training data into new training and validation subsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_train_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Train the RandomForestRegressor model on the new training subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2559\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [10380, 12976]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Validation"
      ],
      "metadata": {
        "id": "ZGfvAL7u20VU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the validation set\n",
        "y_pred = model.predict(X_valid_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_valid, y_pred)\n",
        "r2 = r2_score(y_valid, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'R^2 Score: {r2}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "ZjJjjVvu21tN",
        "outputId": "43a5509e-0874-4e71-a021-03977cdb3246"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-31362c214493>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Make predictions on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ]
}