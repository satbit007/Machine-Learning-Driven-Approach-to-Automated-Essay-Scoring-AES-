{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satbit007/Machine-Learning-Driven-Approach-to-Automated-Essay-Scoring-AES-/blob/main/IML_FinalProject_SatyakiChoudhury.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7cbOOVu2Vo0"
      },
      "source": [
        "Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JwStSwOa1djU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x09YvwE52Ubd"
      },
      "source": [
        "Load and Preprocess the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpu2Bzn52gBe",
        "outputId": "7b52d989-f222-4043-ae09-e15969c621d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names in training data: Index(['essay_id', 'essay_set', 'essay', 'rater1_domain1', 'rater2_domain1',\n",
            "       'rater3_domain1', 'domain1_score', 'rater1_domain2', 'rater2_domain2',\n",
            "       'domain2_score', 'rater1_trait1', 'rater1_trait2', 'rater1_trait3',\n",
            "       'rater1_trait4', 'rater1_trait5', 'rater1_trait6', 'rater2_trait1',\n",
            "       'rater2_trait2', 'rater2_trait3', 'rater2_trait4', 'rater2_trait5',\n",
            "       'rater2_trait6', 'rater3_trait1', 'rater3_trait2', 'rater3_trait3',\n",
            "       'rater3_trait4', 'rater3_trait5', 'rater3_trait6'],\n",
            "      dtype='object')\n",
            "Column names in validation data: Index(['essay_id', 'essay_set', 'essay', 'domain1_predictionid',\n",
            "       'domain2_predictionid'],\n",
            "      dtype='object')\n",
            "   essay_id  essay_set                                              essay  \\\n",
            "0         1          1  Dear local newspaper, I think effects computer...   \n",
            "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
            "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
            "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
            "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
            "\n",
            "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
            "0               4               4             NaN              8   \n",
            "1               5               4             NaN              9   \n",
            "2               4               3             NaN              7   \n",
            "3               5               5             NaN             10   \n",
            "4               4               4             NaN              8   \n",
            "\n",
            "   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n",
            "0             NaN             NaN            NaN  ...            NaN   \n",
            "1             NaN             NaN            NaN  ...            NaN   \n",
            "2             NaN             NaN            NaN  ...            NaN   \n",
            "3             NaN             NaN            NaN  ...            NaN   \n",
            "4             NaN             NaN            NaN  ...            NaN   \n",
            "\n",
            "   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n",
            "0            NaN            NaN            NaN            NaN            NaN   \n",
            "1            NaN            NaN            NaN            NaN            NaN   \n",
            "2            NaN            NaN            NaN            NaN            NaN   \n",
            "3            NaN            NaN            NaN            NaN            NaN   \n",
            "4            NaN            NaN            NaN            NaN            NaN   \n",
            "\n",
            "   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
            "0            NaN            NaN            NaN            NaN  \n",
            "1            NaN            NaN            NaN            NaN  \n",
            "2            NaN            NaN            NaN            NaN  \n",
            "3            NaN            NaN            NaN            NaN  \n",
            "4            NaN            NaN            NaN            NaN  \n",
            "\n",
            "[5 rows x 28 columns]\n",
            "   essay_id  essay_set                                              essay  \\\n",
            "0      1788          1  Dear @ORGANIZATION1, @CAPS1 more and more peop...   \n",
            "1      1789          1  Dear @LOCATION1 Time @CAPS1 me tell you what I...   \n",
            "2      1790          1  Dear Local newspaper, Have you been spending a...   \n",
            "3      1791          1  Dear Readers, @CAPS1 you imagine how life woul...   \n",
            "4      1792          1  Dear newspaper, I strongly believe that comput...   \n",
            "\n",
            "   domain1_predictionid  domain2_predictionid  \n",
            "0                  1788                   NaN  \n",
            "1                  1789                   NaN  \n",
            "2                  1790                   NaN  \n",
            "3                  1791                   NaN  \n",
            "4                  1792                   NaN  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Load training data\n",
        "train_df = pd.read_csv('/content/training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "\n",
        "# Load validation data\n",
        "valid_df = pd.read_csv('/content/valid_set.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "\n",
        "# Load training data\n",
        "train_df = pd.read_csv('/content/training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "\n",
        "# After loading the training data\n",
        "print(\"Column names in training data:\", train_df.columns)\n",
        "\n",
        "# After loading the validation data\n",
        "print(\"Column names in validation data:\", valid_df.columns)\n",
        "\n",
        "# Display the first few rows of the training data\n",
        "print(train_df.head())\n",
        "\n",
        "# Display the first few rows of the validation data\n",
        "print(valid_df.head())\n",
        "\n",
        "# Basic preprocessing\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuations\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "    return text\n",
        "\n",
        "train_df['essay'] = train_df['essay'].apply(preprocess_text)\n",
        "valid_df['essay'] = valid_df['essay'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s2aIWE12oSs"
      },
      "source": [
        "Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69R3JE4g2n-w",
        "outputId": "58349c25-4da5-444d-9ff9-bc65ae5ff91d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the feature set: 12976\n",
            "Number of rows in the target variable: 12976\n"
          ]
        }
      ],
      "source": [
        "train_df = train_df.dropna(subset=['essay', 'domain1_score'])\n",
        "\n",
        "# Reset the index of your DataFrame after preprocessing and before TF-IDF vectorization\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(stop_words=stopwords.words('english'), max_features=5000)\n",
        "X_train_tfidf = tfidf.fit_transform(train_df['essay'])\n",
        "X_valid_tfidf = tfidf.transform(valid_df['essay'])\n",
        "\n",
        "# Continue with TF-IDF vectorization\n",
        "X_train_tfidf = tfidf.fit_transform(train_df['essay'])\n",
        "\n",
        "# Prepare target variable for training data\n",
        "y_train = train_df['domain1_score']\n",
        "\n",
        "# Right after creating X_train_tfidf and y_train\n",
        "print(\"Number of rows in the feature set:\", X_train_tfidf.shape[0])\n",
        "print(\"Number of rows in the target variable:\", y_train.shape[0])\n",
        "\n",
        "\n",
        "# If you decide to use a part of the training data for validation\n",
        "#X_train_tfidf, X_val_tfidf, y_train, y_val = train_test_split(X_train_tfidf, y_train, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW-Koyc12sZy"
      },
      "source": [
        "Model Training - RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "1yoUOT9R2wQy",
        "outputId": "077d3425-d003-4c55-d57a-0ae718c05ddb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor()"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Assuming 'domain1_score' as the target for training data\n",
        "y_train = train_df['domain1_score']\n",
        "\n",
        "# Split the training data into new training and validation subsets\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_tfidf, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the RandomForestRegressor model on the new training subset\n",
        "model = RandomForestRegressor(n_estimators=100)\n",
        "model.fit(X_train_split, y_train_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGfvAL7u20VU"
      },
      "source": [
        "Model Validation - RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjJjjVvu21tN",
        "outputId": "98612553-192b-4937-b596-9310426adf13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on validation set: 6.4674604455572675\n",
            "R^2 Score on validation set: 0.9169324892912425\n"
          ]
        }
      ],
      "source": [
        "# Predict and evaluate the model on the validation subset\n",
        "y_pred_val = model.predict(X_val_split)\n",
        "mse = mean_squared_error(y_val_split, y_pred_val)\n",
        "r2 = r2_score(y_val_split, y_pred_val)\n",
        "\n",
        "# Print the evaluation metrics for the validation subset\n",
        "print(f'Mean Squared Error on validation set: {mse}')\n",
        "print(f'R^2 Score on validation set: {r2}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updated feature extraction with Bag-of-words and Syntactic parsing."
      ],
      "metadata": {
        "id": "eNIpGahrTUPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from collections import Counter\n",
        "from scipy.sparse import hstack\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J1JxSF4TjZ5",
        "outputId": "aa6e7c6c-e022-4ab2-9cf7-c419994337b0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "train_df = pd.read_csv('/content/training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "\n",
        "# Preprocess text function\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing and removing punctuations\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the essays\n",
        "train_df['essay'] = train_df['essay'].apply(preprocess_text)\n",
        "\n",
        "# Drop rows with missing values in 'essay' or 'domain1_score'\n",
        "train_df = train_df.dropna(subset=['essay', 'domain1_score'])\n",
        "\n",
        "# Reset the index of the DataFrame after preprocessing\n",
        "train_df = train_df.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "hfB2qidjTj6H"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['essay'])\n",
        "\n",
        "# Bag-of-Words Vectorization\n",
        "bow_vectorizer = CountVectorizer(stop_words=stopwords.words('english'), max_features=5000)\n",
        "X_train_bow = bow_vectorizer.fit_transform(train_df['essay'])\n",
        "\n",
        "# Function to extract syntactic features (POS tag frequencies)\n",
        "def syntactic_features(text):\n",
        "    # Tokenize the text and perform POS tagging\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    # Count frequencies of each POS tag\n",
        "    counts = Counter(tag for word, tag in pos_tags)\n",
        "    return counts\n",
        "\n",
        "# Apply syntactic feature extraction\n",
        "train_df['syntactic_features'] = train_df['essay'].apply(syntactic_features)\n",
        "\n",
        "# Convert syntactic feature counts to DataFrame\n",
        "syntactic_df = pd.DataFrame.from_records(train_df['syntactic_features']).fillna(0)\n",
        "syntactic_df = syntactic_df.div(syntactic_df.sum(axis=1), axis=0)  # Normalize (optional)\n",
        "\n",
        "# Combine Bag-of-Words, TF-IDF, and Syntactic features\n",
        "X_train_combined = hstack([X_train_bow, X_train_tfidf, syntactic_df])\n"
      ],
      "metadata": {
        "id": "F2IbgDa9bFgB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare target variable for training data\n",
        "y_train = train_df['domain1_score']\n",
        "\n",
        "# Split the combined feature set into training and validation subsets\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_combined, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the RandomForestRegressor model\n",
        "model = RandomForestRegressor(n_estimators=100)\n",
        "model.fit(X_train_split, y_train_split)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "G7qki0-abFWV",
        "outputId": "91e4e99c-4dde-498c-c076-5da13c82da05"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor()"
            ],
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and evaluate the model on the validation subset\n",
        "y_pred_val = model.predict(X_val_split)\n",
        "mse = mean_squared_error(y_val_split, y_pred_val)\n",
        "r2 = r2_score(y_val_split, y_pred_val)\n",
        "\n",
        "# Print the evaluation metrics for the validation subset\n",
        "print(f'Mean Squared Error on validation set: {mse}')\n",
        "print(f'R^2 Score on validation set: {r2}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt09ZU2XbFK3",
        "outputId": "640892d0-d496-4b54-ef6a-c6dc1d982ce1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on validation set: 5.833537182470896\n",
            "R^2 Score on validation set: 0.9250745456498757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model changed to Ridge Regression"
      ],
      "metadata": {
        "id": "M-nE_wzypArz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from collections import Counter\n",
        "from scipy.sparse import hstack\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OJeNY0_pOcc",
        "outputId": "2d755137-ccf0-446e-97ac-1674b7cc9d2b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "train_df = pd.read_csv('/content/training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "\n",
        "# Preprocess text function\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing and removing punctuations\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the essays\n",
        "train_df['essay'] = train_df['essay'].apply(preprocess_text)\n",
        "\n",
        "# Drop rows with missing values in 'essay' or 'domain1_score'\n",
        "train_df = train_df.dropna(subset=['essay', 'domain1_score'])\n",
        "\n",
        "# Reset the index of the DataFrame after preprocessing\n",
        "train_df = train_df.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "CbYsJkcLpOWX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['essay'])\n",
        "\n",
        "# Bag-of-Words Vectorization\n",
        "bow_vectorizer = CountVectorizer(stop_words=stopwords.words('english'), max_features=5000)\n",
        "X_train_bow = bow_vectorizer.fit_transform(train_df['essay'])\n",
        "\n",
        "# Function to extract syntactic features (POS tag frequencies)\n",
        "def syntactic_features(text):\n",
        "    # Tokenize the text and perform POS tagging\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    # Count frequencies of each POS tag\n",
        "    counts = Counter(tag for word, tag in pos_tags)\n",
        "    return counts\n",
        "\n",
        "# Apply syntactic feature extraction\n",
        "train_df['syntactic_features'] = train_df['essay'].apply(syntactic_features)\n",
        "\n",
        "# Convert syntactic feature counts to DataFrame\n",
        "syntactic_df = pd.DataFrame.from_records(train_df['syntactic_features']).fillna(0)\n",
        "syntactic_df = syntactic_df.div(syntactic_df.sum(axis=1), axis=0)  # Normalize (optional)\n",
        "\n",
        "# Combine TF-IDF, Bag-of-Words, and Syntactic features\n",
        "X_train_combined = hstack([X_train_tfidf, X_train_bow, syntactic_df])\n"
      ],
      "metadata": {
        "id": "DbjIG6JIpOOe"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare target variable for training data\n",
        "y_train = train_df['domain1_score']\n",
        "\n",
        "# Split the combined feature set into training and validation subsets\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_combined, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Ridge Regression model\n",
        "model = Ridge(alpha=1.0)\n",
        "model.fit(X_train_split, y_train_split)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "T5s0lKsApOHm",
        "outputId": "6018cf50-6135-4234-895f-9c4cff277f9d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ridge()"
            ],
            "text/html": [
              "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and evaluate the model on the validation subset\n",
        "y_pred_val = model.predict(X_val_split)\n",
        "mse = mean_squared_error(y_val_split, y_pred_val)\n",
        "r2 = r2_score(y_val_split, y_pred_val)\n",
        "\n",
        "# Print the evaluation metrics for the validation subset\n",
        "print(f'Mean Squared Error on validation set: {mse}')\n",
        "print(f'R^2 Score on validation set: {r2}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diL422jYpOAF",
        "outputId": "80e906d8-43a4-4922-e91e-20ef41e8e49b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on validation set: 15.455384644464596\n",
            "R^2 Score on validation set: 0.8014923569661805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model changed to SVM"
      ],
      "metadata": {
        "id": "c42H2PzOphQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from collections import Counter\n",
        "from scipy.sparse import hstack\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzUJBK-wqbdN",
        "outputId": "0ff4c4a1-96cb-47c1-c43b-64a5efb9985e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "train_df = pd.read_csv('/content/training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "\n",
        "# Preprocess text function\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing and removing punctuations\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the essays\n",
        "train_df['essay'] = train_df['essay'].apply(preprocess_text)\n",
        "\n",
        "# Drop rows with missing values in 'essay' or 'domain1_score'\n",
        "train_df = train_df.dropna(subset=['essay', 'domain1_score'])\n",
        "\n",
        "# Reset the index of the DataFrame after preprocessing\n",
        "train_df = train_df.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "N-1b-p4hqbVk"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['essay'])\n",
        "\n",
        "# Bag-of-Words Vectorization\n",
        "bow_vectorizer = CountVectorizer(stop_words=stopwords.words('english'), max_features=5000)\n",
        "X_train_bow = bow_vectorizer.fit_transform(train_df['essay'])\n",
        "\n",
        "# Function to extract syntactic features (POS tag frequencies)\n",
        "def syntactic_features(text):\n",
        "    # Tokenize the text and perform POS tagging\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    # Count frequencies of each POS tag\n",
        "    counts = Counter(tag for word, tag in pos_tags)\n",
        "    return counts\n",
        "\n",
        "# Apply syntactic feature extraction\n",
        "train_df['syntactic_features'] = train_df['essay'].apply(syntactic_features)\n",
        "\n",
        "# Convert syntactic feature counts to DataFrame\n",
        "syntactic_df = pd.DataFrame.from_records(train_df['syntactic_features']).fillna(0)\n",
        "syntactic_df = syntactic_df.div(syntactic_df.sum(axis=1), axis=0)  # Normalize (optional)\n",
        "\n",
        "# Combine TF-IDF, Bag-of-Words, and Syntactic features\n",
        "X_train_combined = hstack([X_train_tfidf, X_train_bow, syntactic_df])\n"
      ],
      "metadata": {
        "id": "IruEvDlBqbLK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare target variable for training data\n",
        "y_train = train_df['domain1_score']\n",
        "\n",
        "# Split the combined feature set into training and validation subsets\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_combined, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the SVM model for regression\n",
        "model = SVR(C=1.0, epsilon=0.2)\n",
        "model.fit(X_train_split, y_train_split)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "JcCtAAILqa_U",
        "outputId": "c426ac7a-92ea-4167-89f6-444e5ab1284e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVR(epsilon=0.2)"
            ],
            "text/html": [
              "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVR(epsilon=0.2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(epsilon=0.2)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and evaluate the model on the validation subset\n",
        "y_pred_val = model.predict(X_val_split)\n",
        "mse = mean_squared_error(y_val_split, y_pred_val)\n",
        "r2 = r2_score(y_val_split, y_pred_val)\n",
        "\n",
        "# Print the evaluation metrics for the validation subset\n",
        "print(f'Mean Squared Error on validation set: {mse}')\n",
        "print(f'R^2 Score on validation set: {r2}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM9jt7ULqa44",
        "outputId": "13e5e431-d3d5-4d01-c3e0-8bb53bda7034"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on validation set: 36.3152770811998\n",
            "R^2 Score on validation set: 0.5335696765016499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble technique - Combining the three models (SVM, Ridge Regression, and RandomForestRegressor) for maximum efficiency."
      ],
      "metadata": {
        "id": "RHEuzpXVtp66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from collections import Counter\n",
        "from scipy.sparse import hstack\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFceXtfNt0dI",
        "outputId": "64e718b8-9f93-4e71-9fd1-8cbc56576f80"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "train_df = pd.read_csv('/content/training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "\n",
        "# Preprocess text function\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing and removing punctuations\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the essays\n",
        "train_df['essay'] = train_df['essay'].apply(preprocess_text)\n",
        "\n",
        "# Drop rows with missing values in 'essay' or 'domain1_score'\n",
        "train_df = train_df.dropna(subset=['essay', 'domain1_score'])\n",
        "\n",
        "# Reset the index of the DataFrame after preprocessing\n",
        "train_df = train_df.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "-4qQk4Rwt0Vg"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['essay'])\n",
        "\n",
        "# Bag-of-Words Vectorization\n",
        "bow_vectorizer = CountVectorizer(stop_words=stopwords.words('english'), max_features=5000)\n",
        "X_train_bow = bow_vectorizer.fit_transform(train_df['essay'])\n",
        "\n",
        "# Function to extract syntactic features (POS tag frequencies)\n",
        "def syntactic_features(text):\n",
        "    # Tokenize the text and perform POS tagging\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    # Count frequencies of each POS tag\n",
        "    counts = Counter(tag for word, tag in pos_tags)\n",
        "    return counts\n",
        "\n",
        "# Apply syntactic feature extraction\n",
        "train_df['syntactic_features'] = train_df['essay'].apply(syntactic_features)\n",
        "\n",
        "# Convert syntactic feature counts to DataFrame\n",
        "syntactic_df = pd.DataFrame.from_records(train_df['syntactic_features']).fillna(0)\n",
        "syntactic_df = syntactic_df.div(syntactic_df.sum(axis=1), axis=0)  # Normalize (optional)\n",
        "\n",
        "# Combine TF-IDF, Bag-of-Words, and Syntactic features\n",
        "X_train_combined = hstack([X_train_tfidf, X_train_bow, syntactic_df])\n"
      ],
      "metadata": {
        "id": "Vl4YGJHqt0P-"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare target variable for training data\n",
        "y_train = train_df['domain1_score']\n",
        "\n",
        "# Split data into training and validation subsets\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_tfidf, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "model_rf = RandomForestRegressor(n_estimators=100)\n",
        "model_ridge = Ridge(alpha=1.0)\n",
        "model_svr = SVR(C=1.0, epsilon=0.2)\n",
        "\n",
        "# Train models\n",
        "model_rf.fit(X_train_split, y_train_split)\n",
        "model_ridge.fit(X_train_split, y_train_split)\n",
        "model_svr.fit(X_train_split, y_train_split)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "MTv2ieS4t0Kt",
        "outputId": "7a4e85ae-0701-42a9-b0ed-fa3e7898a007"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVR(epsilon=0.2)"
            ],
            "text/html": [
              "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVR(epsilon=0.2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(epsilon=0.2)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with each model\n",
        "preds_rf = model_rf.predict(X_val_split)\n",
        "preds_ridge = model_ridge.predict(X_val_split)\n",
        "preds_svr = model_svr.predict(X_val_split)\n",
        "\n",
        "# Average predictions\n",
        "ensemble_preds = (preds_rf + preds_ridge + preds_svr) / 3\n",
        "\n",
        "# Evaluate the ensemble\n",
        "mse = mean_squared_error(y_val_split, ensemble_preds)\n",
        "r2 = r2_score(y_val_split, ensemble_preds)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Mean Squared Error (Ensemble) on validation set: {mse}')\n",
        "print(f'R^2 Score (Ensemble) on validation set: {r2}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJk8C1kzt0Am",
        "outputId": "6c554801-6acc-4811-c28b-519d20fb3566"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Ensemble) on validation set: 7.328811946019666\n",
            "R^2 Score (Ensemble) on validation set: 0.9058693640365971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy has been used for advanced NLP processing."
      ],
      "metadata": {
        "id": "HvQqO0Nv9cjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from scipy.sparse import hstack\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0AEeUi_9hAd",
        "outputId": "edc01afa-0e5a-43c1-82dc-b609b7fa174d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "train_df = pd.read_csv('/content/training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "\n",
        "# Define the preprocess_text function\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing and removing punctuations\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the essays\n",
        "train_df['essay'] = train_df['essay'].apply(preprocess_text)\n",
        "\n",
        "# Drop rows with missing values in 'essay' or 'domain1_score'\n",
        "train_df = train_df.dropna(subset=['essay', 'domain1_score'])\n",
        "\n",
        "# Reset the index of the DataFrame after preprocessing\n",
        "train_df = train_df.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "UA5mujvz9h9M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['essay'])\n",
        "\n",
        "bow_vectorizer = CountVectorizer(stop_words=stopwords.words('english'), max_features=5000)\n",
        "X_train_bow = bow_vectorizer.fit_transform(train_df['essay'])\n",
        "\n",
        "def syntactic_features(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    counts = Counter(tag for word, tag in pos_tags)\n",
        "    return counts\n",
        "\n",
        "train_df['syntactic_features'] = train_df['essay'].apply(syntactic_features)\n",
        "syntactic_df = pd.DataFrame.from_records(train_df['syntactic_features']).fillna(0)\n",
        "syntactic_df = syntactic_df.div(syntactic_df.sum(axis=1), axis=0)  # Normalize (optional)\n",
        "\n",
        "# Load spacy model for advanced NLP processing\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def advanced_nlp_features(text):\n",
        "    doc = nlp(text)\n",
        "    grammatical_errors = sum(1 for token in doc if token.tag_ in ['WRONG_TAG1', 'WRONG_TAG2'])\n",
        "    # Additional features based on insights can be added here\n",
        "    return {'grammatical_errors': grammatical_errors}\n",
        "\n",
        "train_df['advanced_nlp_features'] = train_df['essay'].apply(advanced_nlp_features)\n",
        "advanced_nlp_df = pd.DataFrame.from_records(train_df['advanced_nlp_features'])\n",
        "\n",
        "X_train_combined = hstack([X_train_tfidf, X_train_bow, syntactic_df, advanced_nlp_df])"
      ],
      "metadata": {
        "id": "qv8zOT6l9h2m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train_df['domain1_score']\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_combined, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100)\n",
        "model.fit(X_train_split, y_train_split)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "WbntryCk9hvl",
        "outputId": "ac235122-8d39-4c7e-b68b-a0058d5890f4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_val = model.predict(X_val_split)\n",
        "mse = mean_squared_error(y_val_split, y_pred_val)\n",
        "r2 = r2_score(y_val_split, y_pred_val)\n",
        "\n",
        "print(f'Mean Squared Error on validation set: {mse}')\n",
        "print(f'R^2 Score on validation set: {r2}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mojXN9k9hnP",
        "outputId": "45a46ea1-7241-4b6e-e77f-196067ccb828"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on validation set: 5.836116231852422\n",
            "R^2 Score on validation set: 0.9250414205594446\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeruwgjlefxFCi/HufZRr+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}